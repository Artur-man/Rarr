#' Print a summary of a Zarr array
#'
#' When reading a Zarr array using [read_zarr_array()] it is necessary to know
#' it's shape and size. `zarr_overview()` can be used to get a quick
#' overview of the array shape and contents, based on the .zarray metadata file
#' each array contains.
#'
#' The function currently prints the following information to the R console: 
#'  - array path
#'  - array shape and size
#'  - chunk and size
#'  - the number of chunks
#'  - the datatype of the array
#'  - codec used for data compression (if any)
#'  
#' If given the path to a group of arrays the function will attempt to print
#' the details of all sub-arrays in the group.
#'
#' @param zarr_array_path A character vector of length 1.  This provides the
#'   path to a Zarr array or group of arrays. This can either be on a local file system or
#'   on S3 storage.
#'
#' @return The function invisible returns `TRUE` if successful.  However it is
#'   primarily called for the side effect of printing details of the Zarr array(s)
#'   to the screen.
#'   
#' @examples
#'
#' ## Using a local file provided with the package
#' z1 <- system.file("extdata", "zarr_examples", "row-first",
#'                   "int32.zarr", package = "Rarr")
#'
#' ## read the entire array
#' zarr_overview(zarr_array_path = z1)  
#' 
#' ## using a file on S3 storage
#' z2 <- "https://uk1s3.embassy.ebi.ac.uk/idr/zarr/v0.4/idr0101A/13457539.zarr/1"
#' zarr_overview(z2)
#'
#' @export
zarr_overview <- function(zarr_array_path) {
  
  zarr_array_path <- .normalize_array_path(zarr_array_path)
  s3_provider <- s3_provider(path = zarr_array_path)
  
  dot_zmeta <- .read_zmetadata(zarr_path = zarr_array_path, s3_provider = s3_provider)
  if(!is.null(dot_zmeta)) {
    
    arrays <- grep(names(dot_zmeta$metadata), pattern = "/.zarray", fixed = TRUE, value = TRUE)
    cat("Type: Group of Arrays\n")
    cat("Path:", normalizePath(zarr_array_path, mustWork = FALSE), "\n")
    cat("Arrays:\n")
    for(a in arrays) {
      cat("---\n")
      .print_array_metadata(dirname(a), dot_zarray = dot_zmeta$metadata[[ a ]], indent = "  ")
    }
    
  } else {
    dot_zarray <- read_array_metadata(path = zarr_array_path, s3_provider = s3_provider)
    cat("Type: Array\n")
    .print_array_metadata(zarr_array_path, dot_zarray = dot_zarray)
    
  }
  invisible(TRUE)
}

.print_array_metadata <- function(zarr_array_path , dot_zarray, indent = "") {
  dt <- .parse_datatype(dot_zarray$dtype)
  nchunks <- ceiling(unlist(dot_zarray$shape) / unlist(dot_zarray$chunks))
  
  cat(indent, "Path: ", normalizePath(zarr_array_path, mustWork = FALSE), "\n", sep = "")
  cat(indent, "Shape: ", paste(unlist(dot_zarray$shape), collapse = " x "), "\n", sep = "")
  cat(indent, "Chunk Shape: ", paste(unlist(dot_zarray$chunks), collapse = " x "), "\n", sep = "")
  cat(indent, "No. of Chunks: ", prod(nchunks), " (", paste(nchunks, collapse = " x "), ")", "\n", sep = "")
  cat(indent, "Data Type: ", dt$base_type, 8 * dt$nbytes, "\n", sep = "")
  cat(indent, "Endianness: ", dt$endian, "\n", sep = "")
  if(is.null(dot_zarray$compressor)) {
    cat(indent, "Compressor: None\n", sep = "")
  } else {
    cat(indent, "Compressor: ", dot_zarray$compressor$id, "\n", sep = "")
  }
}


#' @import jsonlite
#' @importFrom httr2 url_parse
#' @importFrom stringr str_extract str_remove
#' @importFrom aws.s3 s3read_using
#' 
#' @keywords Internal
read_array_metadata <- function(path, s3_provider = NULL) {
  
  zarray_path <- paste0(path, ".zarray")
  
  if(!is.null(s3_provider)) {
    if(s3_provider == "aws") {
      parsed_url <- url_parse_aws(zarray_path)
    } else {
      parsed_url <- .url_parse_other(zarray_path)
    }
    metadata <- s3read_using(FUN = read_json, 
                             object = parsed_url$object, 
                             bucket = parsed_url$bucket, 
                             opts = list(region = parsed_url$region, 
                                         base_url = parsed_url$hostname))

  } else {
    metadata <- read_json(zarray_path)
  }
  
  metadata <- update_fill_value(metadata)
  
  return(metadata)
}

#' Convert special fill values from strings to numbers
#' 
#' Special case fill values (NaN, Inf, -Inf) are encoded as strings in the Zarra
#' metadata.  R will create arrays of type character if these are defined and
#' the chunk isn't present on disk. This function updates the fill value to be
#' R's representation of these special values, so numeric arrays are created.
#'
#' @param metadata A list containing the array metadata.  This should normally
#'   be generated by running `read_json()` on the `.zarray` file.
#'
#' @keywords Internal
update_fill_value <- function(metadata) {
  
  if(metadata$fill_value %in% c("NaN", "Infinity", "-Infinity")) {
    datatype <- .parse_datatype(metadata$dtype)
    if(datatype$base_type != "string") {
      metadata$fill_value <- switch(
        metadata$fill_value,
        "NaN" = NaN,
        "Infinity" = Inf,
        "-Infinity" = -Inf
      )
    }
  }
  return(metadata)
  
}

#' @import jsonlite
#' @importFrom aws.s3 object_exists
#' @keywords Internal
.read_zmetadata <- function(zarr_path, s3_provider) {
  
  zmeta_path <- paste0(zarr_path, ".zmetadata")
  zmeta <- NULL
  
  if(!is.null(s3_provider)) {
    
    if(s3_provider == "aws") {
      parsed_url <- url_parse_aws(zmeta_path)
    } else {
      parsed_url <- .url_parse_other(zmeta_path)
    }
    zmeta_exists <- suppressMessages(
      object_exists(object = parsed_url$object, bucket = parsed_url$bucket, 
                    region = parsed_url$region, base_url = parsed_url$hostname)
    )
    if(zmeta_exists) {
      zmeta <- s3read_using(FUN = read_json, 
                            object = parsed_url$object, 
                            bucket = parsed_url$bucket, 
                            opts = list(region = parsed_url$region, 
                                        base_url = parsed_url$hostname))
    }
  } else {
    if(file.exists(zmeta_path)) {
      zmeta <- read_json(zmeta_path)
    }
  }
  
  return(zmeta)
}

